#!/usr/bin/env python3
"""
å°çº¢ä¹¦å†…å®¹è·å–å·¥å…·
ä½¿ç”¨requestså’ŒBeautifulSoupè·å–å°çº¢ä¹¦é“¾æ¥å†…å®¹
"""

import requests
from bs4 import BeautifulSoup
import re
import json
from pathlib import Path
from datetime import datetime
from urllib.parse import urlparse, parse_qs
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from src.core.content_manager import ContentManager
from src.utils.download_images_from_urls import download_multiple_files
 
def extract_xhs_content(url):
    """
    æå–å°çº¢ä¹¦é“¾æ¥å†…å®¹
    
    Args:
        url: å°çº¢ä¹¦é“¾æ¥ï¼ˆæ”¯æŒçŸ­é“¾æ¥å’ŒåŸå§‹é“¾æ¥ï¼‰
    
    Returns:
        dict: åŒ…å«æ ‡é¢˜ã€å†…å®¹ã€å›¾ç‰‡URLç­‰ä¿¡æ¯çš„å­—å…¸
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',
        'Accept-Encoding': 'gzip, deflate, br',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
        'Cache-Control': 'max-age=0'
    }
    
    try:
        # å¤„ç†çŸ­é“¾æ¥é‡å®šå‘
        print(f"æ­£åœ¨è§£æé“¾æ¥: {url}")
        response = requests.get(url, headers=headers, allow_redirects=True, timeout=30)
        response.raise_for_status()
        
        # è·å–æœ€ç»ˆé‡å®šå‘çš„URL
        final_url = response.url
        print(f"é‡å®šå‘åˆ°: {final_url}")
        
        # è§£æå°çº¢ä¹¦ç¬”è®°ID
        note_id = extract_note_id(final_url)
        if not note_id:
            return {"error": "æ— æ³•è§£æå°çº¢ä¹¦ç¬”è®°ID"}
        
        print(f"è§£æåˆ°ç¬”è®°ID: {note_id}")
        
        # è·å–é¡µé¢å†…å®¹
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æå–æ ‡é¢˜
        title = extract_title(soup)
        
        # æå–å†…å®¹
        content = extract_content(soup)
        
        # æå–å›¾ç‰‡URL
        image_urls = extract_image_urls(soup)
        
        # æå–æ ‡ç­¾
        tags = extract_tags(soup)
        
        # æå–ä½œè€…ä¿¡æ¯
        author_info = extract_author_info(soup)
        
        result = {
            "note_id": note_id,
            "title": title,
            "content": content,
            "image_urls": image_urls,
            "tags": tags,
            "author": author_info,
            "url": final_url,
            "original_url": url,
            "extraction_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        
        return result
        
    except requests.RequestException as e:
        return {"error": f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"}
    except Exception as e:
        return {"error": f"è§£æå¤±è´¥: {str(e)}"}

def extract_note_id(url):
    """ä»URLä¸­æå–å°çº¢ä¹¦ç¬”è®°ID"""
    # åŒ¹é…å°çº¢ä¹¦ç¬”è®°URLæ¨¡å¼
    patterns = [
        r'/explore/([a-f0-9]+)',  # æ ‡å‡†æ ¼å¼
        r'/discovery/item/([a-f0-9]+)',  # å‘ç°é¡µæ ¼å¼
        r'noteId=([a-f0-9]+)',  # å‚æ•°æ ¼å¼
    ]
    
    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    
    # å¦‚æœæ— æ³•åŒ¹é…ï¼Œå°è¯•ä»è·¯å¾„ä¸­æå–
    parsed_url = urlparse(url)
    path_parts = parsed_url.path.split('/')
    for part in path_parts:
        if len(part) >= 8 and re.match(r'^[a-f0-9]+$', part):
            return part
    
    return None

def extract_title(soup):
    """æå–æ ‡é¢˜"""
    # å°è¯•å¤šç§é€‰æ‹©å™¨
    selectors = [
        'meta[property="og:title"]',
        'title',
        '.note-title',
        'h1',
        '.title'
    ]
    
    for selector in selectors:
        element = soup.select_one(selector)
        if element:
            title = element.get('content') or element.get_text(strip=True)
            if title and len(title) > 5:
                return title
    
    return "æœªæ‰¾åˆ°æ ‡é¢˜"

def extract_content(soup):
    """æå–å†…å®¹"""
    # å°è¯•å¤šç§é€‰æ‹©å™¨
    selectors = [
        '.note-content',
        '.content',
        '.desc',
        'meta[property="og:description"]',
        'meta[name="description"]',
        'article'
    ]
    
    for selector in selectors:
        element = soup.select_one(selector)
        if element:
            content = element.get('content') or element.get_text(strip=True)
            if content and len(content) > 10:
                # æ¸…ç†å†…å®¹
                content = re.sub(r'\s+', ' ', content)
                return content.strip()
    
    return "æœªæ‰¾åˆ°å†…å®¹"

def extract_image_urls(soup):
    """æå–å›¾ç‰‡URL"""
    image_urls = []
    
    # é¦–å…ˆå°è¯•ä»JSONæ•°æ®ä¸­æå–å›¾ç‰‡ï¼ˆå°çº¢ä¹¦å¸¸ç”¨æ–¹æ³•ï¼‰
    script_tags = soup.find_all('script')
    for script in script_tags:
        script_content = script.string
        if script_content and 'imageList' in script_content:
            # å°è¯•è§£æJSONæ•°æ®
            import json
            import re
            
            # æŸ¥æ‰¾JSONæ•°æ®
            json_pattern = r'\{"imageList":\[.*?\]\}'
            matches = re.findall(json_pattern, script_content, re.DOTALL)
            for match in matches:
                try:
                    data = json.loads(match)
                    if 'imageList' in data:
                        for img_info in data['imageList']:
                            if isinstance(img_info, dict) and 'url' in img_info:
                                image_urls.append(img_info['url'])
                            elif isinstance(img_info, str):
                                image_urls.append(img_info)
                except:
                    pass
    
    # å°è¯•å¤šç§é€‰æ‹©å™¨
    selectors = [
        'img[src*="xiaohongshu"]',
        'img[src*="xhscdn"]',
        'img[src*="sns-img"]',
        '.note-image img',
        '.image img',
        '.content-image img',
        'img[alt*="å°çº¢ä¹¦"]',
        'img[data-src*="xiaohongshu"]',
        'img[data-src*="xhscdn"]',
        'img[data-src*="sns-img"]',
        'img',  # æ‰€æœ‰å›¾ç‰‡
        'div[style*="background-image"]'  # èƒŒæ™¯å›¾ç‰‡
    ]
    
    for selector in selectors:
        images = soup.select(selector)
        for img in images:
            # å°è¯•å¤šä¸ªå±æ€§
            for attr in ['src', 'data-src', 'data-original', 'original', 'url']:
                src = img.get(attr)
                if src and src.startswith(('http://', 'https://', '//')):
                    # å¤„ç†ç›¸å¯¹è·¯å¾„
                    if src.startswith('//'):
                        src = 'https:' + src
                    image_urls.append(src)
                    break  # æ‰¾åˆ°ä¸€ä¸ªæœ‰æ•ˆURLå°±åœæ­¢
            
            # æ£€æŸ¥èƒŒæ™¯å›¾ç‰‡
            style = img.get('style', '')
            if 'background-image' in style:
                import re
                bg_match = re.search(r'background-image:\s*url\(["\']?(.*?)["\']?\)', style)
                if bg_match:
                    bg_url = bg_match.group(1)
                    if bg_url.startswith(('http://', 'https://', '//')):
                        if bg_url.startswith('//'):
                            bg_url = 'https:' + bg_url
                        image_urls.append(bg_url)
    
    # å»é‡å¹¶è¿‡æ»¤æ— æ•ˆURL
    unique_urls = []
    for url in set(image_urls):
        # è¿‡æ»¤æ‰å¯èƒ½ä¸æ˜¯å›¾ç‰‡çš„URL
        if any(keyword in url.lower() for keyword in ['xiaohongshu', 'xhscdn', 'sns-img', 'alicdn', 'cdn']):
            # æ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡URLï¼ˆåŒ…å«å¸¸è§å›¾ç‰‡æ‰©å±•åæˆ–å›¾ç‰‡å…³é”®è¯ï¼‰
            if any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp', 'image', 'img']):
                unique_urls.append(url)
    
    return unique_urls

def extract_tags(soup):
    """æå–æ ‡ç­¾"""
    tags = []
    
    # ä»å†…å®¹ä¸­æå–æ ‡ç­¾
    content = extract_content(soup)
    if content:
        hashtags = re.findall(r'#([^#\s]+)', content)
        tags.extend(hashtags)
    
    # ä»metaæ ‡ç­¾ä¸­æå–
    meta_keywords = soup.find('meta', {'name': 'keywords'})
    if meta_keywords:
        keywords = meta_keywords.get('content', '')
        if keywords:
            tags.extend([tag.strip() for tag in keywords.split(',') if tag.strip()])
    
    return list(set(tags))

def extract_author_info(soup):
    """æå–ä½œè€…ä¿¡æ¯"""
    # å°è¯•æå–ä½œè€…å
    selectors = [
        '.author-name',
        '.user-name',
        '.nickname',
        'meta[property="og:article:author"]'
    ]
    
    for selector in selectors:
        element = soup.select_one(selector)
        if element:
            author = element.get('content') or element.get_text(strip=True)
            if author:
                return author
    
    return "æœªçŸ¥ä½œè€…"

def save_xhs_content(content_data, account_name="AIçŸ¥è¯†è´¦å·", download_images=True):
    """
    ä¿å­˜å°çº¢ä¹¦å†…å®¹åˆ°é¡¹ç›®ç›®å½•
    
    Args:
        content_data: æå–çš„å†…å®¹æ•°æ®
        account_name: è´¦å·åç§°
        download_images: æ˜¯å¦ä¸‹è½½å›¾ç‰‡
    
    Returns:
        Path: ä¿å­˜çš„ç›®å½•è·¯å¾„
    """
    if "error" in content_data:
        print(f"âŒ ä¿å­˜å¤±è´¥: {content_data['error']}")
        return None
    
    # åˆ›å»ºå†…å®¹ç®¡ç†å™¨
    manager = ContentManager()
    
    # ç”Ÿæˆå¸–å­æ ‡é¢˜
    title = content_data.get('title', f"å°çº¢ä¹¦ç¬”è®°_{content_data.get('note_id', 'unknown')}")
    
    # åˆ›å»ºå¸–å­ç›®å½•
    post_dir = manager.create_post_directory(
        content_data.get('note_id', 'unknown'),
        title,
        account_name
    )
    
    print(f"ğŸ“ åˆ›å»ºç›®å½•: {post_dir}")
    
    # å‡†å¤‡å¸–å­ä¿¡æ¯
    post_info = {
        "title": title,
        "post_id": content_data.get('note_id', 'unknown'),
        "url": content_data.get('url', ''),
        "original_url": content_data.get('original_url', ''),
        "author": content_data.get('author', 'æœªçŸ¥ä½œè€…'),
        "publish_time": datetime.now().strftime("%Y-%m-%d_%H:%M:%S"),
        "tags": content_data.get('tags', []),
        "description": content_data.get('content', ''),
        "extraction_time": content_data.get('extraction_time', ''),
        "image_urls": content_data.get('image_urls', [])
    }
    
    # ä¿å­˜å¸–å­ä¿¡æ¯
    info_path = manager.save_post_info(post_dir, post_info)
    print(f"ğŸ’¾ ä¿å­˜å¸–å­ä¿¡æ¯åˆ°: {info_path}")
    
    # ä¿å­˜åŸå§‹å†…å®¹
    raw_content_path = post_dir / "raw_content.json"
    with open(raw_content_path, 'w', encoding='utf-8') as f:
        json.dump(content_data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“„ ä¿å­˜åŸå§‹å†…å®¹åˆ°: {raw_content_path}")
    
    # ä¿å­˜ä¸ºMarkdownæ ¼å¼
    md_content = generate_markdown_content(content_data)
    md_path = post_dir / "content.md"
    with open(md_path, 'w', encoding='utf-8') as f:
        f.write(md_content)
    print(f"ğŸ“ ä¿å­˜Markdownå†…å®¹åˆ°: {md_path}")
    
    # ä¸‹è½½å›¾ç‰‡
    if download_images and content_data.get('image_urls'):

        image_urls = content_data.get('image_urls', [])
        if image_urls:
            print(f"\nğŸ“· å¼€å§‹ä¸‹è½½ {len(image_urls)} å¼ å›¾ç‰‡...")
            downloads_dir = post_dir / "downloads"
            results = download_multiple_files(image_urls, downloads_dir, "image_{:02d}")
            
            print(f"ğŸ“Š å›¾ç‰‡ä¸‹è½½å®Œæˆ:")
            print(f"   æˆåŠŸ: {results['success']}/{results['total']}")
            print(f"   å¤±è´¥: {results['failed']}/{results['total']}")
            
            if results['failed_urls']:
                print("\nâŒ ä¸‹è½½å¤±è´¥çš„URL:")
                for url in results['failed_urls']:
                    print(f"  - {url}")
        else:
            print("\nâ„¹ï¸  æœªå‘ç°å¯ä¸‹è½½çš„å›¾ç‰‡")
    
    return post_dir

def generate_markdown_content(content_data):
    """ç”ŸæˆMarkdownæ ¼å¼çš„å†…å®¹"""
    lines = []
    
    # æ ‡é¢˜
    lines.append(f"# {content_data.get('title', 'å°çº¢ä¹¦ç¬”è®°')}")
    lines.append("")
    
    # å…ƒä¿¡æ¯
    lines.append("## åŸºæœ¬ä¿¡æ¯")
    lines.append(f"- **ç¬”è®°ID**: {content_data.get('note_id', 'æœªçŸ¥')}")
    lines.append(f"- **ä½œè€…**: {content_data.get('author', 'æœªçŸ¥ä½œè€…')}")
    lines.append(f"- **æå–æ—¶é—´**: {content_data.get('extraction_time', 'æœªçŸ¥')}")
    lines.append(f"- **åŸå§‹é“¾æ¥**: {content_data.get('original_url', '')}")
    lines.append(f"- **é‡å®šå‘é“¾æ¥**: {content_data.get('url', '')}")
    lines.append("")
    
    # å†…å®¹
    lines.append("## å†…å®¹")
    content = content_data.get('content', '')
    if content:
        lines.append(content)
    else:
        lines.append("*å†…å®¹ä¸ºç©º*")
    lines.append("")
    
    # æ ‡ç­¾
    tags = content_data.get('tags', [])
    if tags:
        lines.append("## æ ‡ç­¾")
        lines.append(" ".join([f"#{tag}" for tag in tags]))
        lines.append("")
    
    # å›¾ç‰‡ä¿¡æ¯
    image_urls = content_data.get('image_urls', [])
    if image_urls:
        lines.append("## å›¾ç‰‡")
        lines.append(f"å…±å‘ç° {len(image_urls)} å¼ å›¾ç‰‡")
        lines.append("")
        
        # æ·»åŠ å·²ä¸‹è½½å›¾ç‰‡çš„å¼•ç”¨
        for i in range(len(image_urls)):
            image_filename = f"image_{i+1:02d}"
            lines.append(f"![å›¾ç‰‡{i+1}](./downloads/{image_filename}.jpg)")
            lines.append(f"*å›¾{i+1}: {image_urls[i]}*")
            lines.append("")
        
        # åŸå§‹å›¾ç‰‡é“¾æ¥
        lines.append("### åŸå§‹å›¾ç‰‡é“¾æ¥")
        for i, url in enumerate(image_urls, 1):
            lines.append(f"{i}. {url}")
    
    return '\n'.join(lines)

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='å°çº¢ä¹¦å†…å®¹è·å–å·¥å…·')
    parser.add_argument('url', help='å°çº¢ä¹¦é“¾æ¥')
    parser.add_argument('--account', '-a', default='AIçŸ¥è¯†è´¦å·', 
                       help='è´¦å·åç§°ï¼Œé»˜è®¤ä¸ºAIçŸ¥è¯†è´¦å·')
    parser.add_argument('--output', '-o', help='è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('--no-download', action='store_true',
                       help='ä¸ä¸‹è½½å›¾ç‰‡ï¼Œä»…æå–å†…å®¹')
    
    args = parser.parse_args()
    
    print("=== å°çº¢ä¹¦å†…å®¹è·å–å·¥å…· ===")
    print(f"ç›®æ ‡é“¾æ¥: {args.url}")
    print(f"è´¦å·åç§°: {args.account}")
    print(f"ä¸‹è½½å›¾ç‰‡: {'å¦' if args.no_download else 'æ˜¯'}")
    print("=" * 40)
    
    # æå–å†…å®¹
    content_data = extract_xhs_content(args.url)
    
    if "error" in content_data:
        print(f"âŒ æå–å¤±è´¥: {content_data['error']}")
        return 1
    
    # æ˜¾ç¤ºæå–ç»“æœ
    print("\nâœ… å†…å®¹æå–æˆåŠŸ!")
    print(f"æ ‡é¢˜: {content_data.get('title', 'æœªçŸ¥')}")
    print(f"ä½œè€…: {content_data.get('author', 'æœªçŸ¥')}")
    print(f"ç¬”è®°ID: {content_data.get('note_id', 'æœªçŸ¥')}")
    print(f"å†…å®¹é•¿åº¦: {len(content_data.get('content', ''))} å­—ç¬¦")
    print(f"å›¾ç‰‡æ•°é‡: {len(content_data.get('image_urls', []))}")
    print(f"æ ‡ç­¾æ•°é‡: {len(content_data.get('tags', []))}")
    
    # ä¿å­˜å†…å®¹
    save_dir = save_xhs_content(content_data, args.account, not args.no_download)
    
    if save_dir:
        print(f"\nğŸ‰ å†…å®¹å·²ä¿å­˜åˆ°: {save_dir}")
        print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print(f"  - post_info.json (å¸–å­ä¿¡æ¯)")
        print(f"  - raw_content.json (åŸå§‹æ•°æ®)")
        print(f"  - content.md (Markdownæ ¼å¼)")
        
        # å¦‚æœä¸‹è½½äº†å›¾ç‰‡ï¼Œæ˜¾ç¤ºå›¾ç‰‡ä¿¡æ¯
        if not args.no_download and content_data.get('image_urls'):
            downloads_dir = save_dir / "downloads"
            if downloads_dir.exists():
                image_files = list(downloads_dir.glob("*"))
                if image_files:
                    print(f"  - downloads/ (å›¾ç‰‡ç›®å½•ï¼ŒåŒ…å« {len(image_files)} å¼ å›¾ç‰‡)")
        
        # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
        content = content_data.get('content', '')
        if content:
            preview = content[:200] + "..." if len(content) > 200 else content
            print(f"\nğŸ“ å†…å®¹é¢„è§ˆ: {preview}")
    
    return 0

if __name__ == "__main__":
    exit(main())